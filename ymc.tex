\documentclass{article}
\usepackage[utf8]{inputenc}

\newcommand{\nirpdftitle}{YMC Plenary Talks}
\usepackage{import}
\inputfrom{../notes}{nir}
% \usepackage[backend=biber,
%     style=alphabetic,
%     sorting=ynt
% ]{biblatex}
% \addbibresource{bib.bib}
\setcounter{tocdepth}{2}

\pagestyle{contentpage}

\title{YMC Plenary Talks}
\author{Nir Elber}
\date{August 2022}
\usepackage{graphicx}
\lhead{}
\rhead{\textit{YMC PLENARY TALKS}}

\begin{document}

\maketitle

\tableofcontents

\section{Newton's Method for the Clueless --- Jeff Diller}
Math is more than numbers.
\begin{idea}
	For this talk, math is about solving equations.
\end{idea}
To be specific, we are interested in solving for $x$ solving $f(x)=0$, where $f$ is some specified function (often polynomial).
\begin{example}
	Here are some sets meant to solve equations in $\ZZ[x]$.
	\begin{itemize}
		\item Integers solve equations $x-k\in\ZZ[x]$.
		\item Rational numbers solve equations like $ax+b\in\ZZ[x]$.
		\item Real numbers solve equations like $x^2-2\in\ZZ[x]$.
		\item Complex numbers solve equations like $x^2+1\in\ZZ[x]$.
	\end{itemize}
\end{example}
It will be difficult to ``algebraically'' solving polynomials arbitrarily; this is impossible for degree at least $5$. In this talk, we will be interested in numerical approximations of these solutions. For this, we will use Newton's method.
\begin{remark}
	Newton's method is attributed to Newton, but it's probably due to Simpson. Other mathematicians who have a claim to fame here are Joseph Raphson and Seki Takakazu.
\end{remark}

\subsection{Newton's Method}
We follow the following idea.
\begin{idea}
	If we want to solve $f(x)=0$, guess a solution $x_0$ and pretend $f$ is linear.
\end{idea}
To be a little more technical, we begin with a guess $x_0$, approximate
\[f(x)\approx f(x_0)+f'(x_0)(x-x_0)\]
and then set
\[x_1\coloneqq x_0-\frac{f(x_0)}{f'(x_0)}\]
to be our new guess. Here is the image. 
\begin{center}
	\begin{asy}
		import graph;
		unitsize(3cm);
		real f(real x)
		{
			return x*x*x - x + 0.1;
		}
		draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
		draw(graph(f,-0.6,1.5), blue);

		real a0 = 1.3;
		real a1 = a0 - (a0*a0*a0 - a0) / (3*a0*a0 - 1);

		pair v = (a0, f(a0)) - (a1, 0);
		draw((a1, 0) -- (a1, 0) + 1.8*v, red);

		draw((a0,0) -- (a0,f(a0)), dashed);
		draw((a1,0) -- (a1,f(a1)), dashed);

		dot("$x_0$", (a0,0), S); dot("$x_1$", (a1, 0), S);

		dot("$(x_0,f(x_0))$", (a0, f(a0)), WNW);
		dot("$(x_{1},f(x_{1}))$", (a1, f(a1)), WNW);

		label("\color{red}$y-f(x_0)=f'(x_0)(x-x_0)$", (a1,0) + 1.5*v, W);
	\end{asy}
\end{center}
In practice, we will take our guess $x_1$ and repeat the process, and we will end up well-approximating the root.
\begin{remark}
	This is an example of a ``dynamical system,'' where we have some function $N\colon X\to X$ and ask what happens to the orbits of points $x\in X$. In particular, Newton's method has
	\[N(x)=x-\frac{f(x)}{f'(x)}.\]
\end{remark}
\begin{example}
	With $f(x)=x^2-2$, we have
	\[N(x)\coloneqq\frac12\left(x+\frac2x\right)\]
	as our iteration function. In particular, for any field $K$, we have $N\colon K\to K$.
\end{example}
\begin{example}
	With $f(x)=x^2+1$, we have
	\[N(x)\coloneqq\frac12\left(x+\frac{-1}x\right)\]
	as our iteration function. Because $N\colon\RR\to\RR$, we expect applying Newton's method to guesses starting as real numbers to not be very helpful.
\end{example}
\begin{remark}
	From the perspective of dynamical systems, $f(x)=x^2+1$ is actually very interesting: a random first guess will provide a sequence dense in $\RR$. However, we can find periodic points as well, which is pretty cool.
\end{remark}

\subsection{Efficiency}
To justify Newton's method's efficiency, let's try another method and compare. The idea for our second method is the Intermediate value theorem.
\begin{theorem}[Intermediate value]
	Let $f\colon\RR\to\RR$ be continuous. Then if $f(a)<0$ and $f(b)>0$, there is a root of $f$ between $a$ and $b$.
\end{theorem}
As such, we can imagine taking an interval $[a,b]$, evaluating $f\left(\frac{a+b}2\right)$ and dividing the interval in half depending on its sign. Here is the image.
\begin{center}
	\begin{asy}
		import graph;
		unitsize(3cm);
		real f(real x)
		{
			return x*x*x - x + 0.1;
		}
		draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
		draw(graph(f,-0.6,1.5), blue);

		real a0 = 0.3;
		real b0 = 1.3;
		real a1 = (a0+b0)/2;

		draw((a0,0) -- (a0,f(a0)), dashed);
		draw((a1,0) -- (a1,f(a1)), dashed);
		draw((b0,0) -- (b0,f(b0)), dashed);

		dot("$b_0$", (b0,0), S); dot("$a_0$", (a0, 0), N);
		dot("$a_1$", (a1, 0), N);

		dot("$(a_0,f(a_0))$", (a0, f(a0)), SW);
		dot("$(a_1,f(a_1))$", (a1, f(a1)), SE);
		dot("$(b_0,f(b_0))$", (b0, f(b0)), WNW);
	\end{asy}
\end{center}
This ``Intermediate value theorem'' method uses a single iteration to get a bit of information out of the root. However, Newton's method gets a quadratic improvement on the root!

Let's see this quadratic improvement. Without loss of generality, suppose $f(0)=0$ is our root. Then, using a Taylor series, we have
\[N(x)=x^2\cdot\frac{f''(0)}{f'(0)}+O\left(x^3\right),\]
so with $x$ small we get $N(x)\sim x^2$ to be a quadratic improvement.

\subsection{Random Guess}
One issue with this method is that we need a good first guess. What if we just tried guessing? This idea is due to Cayley.
\begin{example}
	Take $f(x)=x^2-1$. It turns out that, guessing $x_0\in\CC$, we have that $x_n\to1$ if $\op{Re}x_0>0$ and $x_n\to-1$ if $\op{Re}x_0<0$.
\end{example}
\begin{remark}
	In the case of $\op{Re}x_0=0$, this is essentially analogous to trying to solve $f(x)=x^2+1$ over $\RR$.
\end{remark}
The behavior for quadratic polynomials is essentially isomorphic to this behavior for $f(x)=x^2-1$, essentially applying a suitable isometry and dilation.

So what happens with cubic polynomials? Well, the behavior becomes fractal-like and quite complicated, and it stays complicated as one adds more roots. However, we still can gain some control over this. We pick up the following definition.
\begin{definition}
	Let $N\colon\CC\to\CC$ be Newton's method for a given polynomial $f$. The set of all points $z\in\CC$ which approach a given root $\alpha$ is called the \textit{basin} of $\alpha$. The connected component of the basin of $\alpha$ which contains $\alpha$ is called the \textit{immediate basin}.
\end{definition}
\begin{remark}
	All these basins are open: a small perturbed element of the basin will still converge to the same root.
\end{remark}
There is the following result.
\begin{theorem}
	The immediate basin of a given root $\alpha$ is simply connected in $\CC$; i.e., it is homeomorphic to the disk $\overline{B(0,1)}$ of radius $1$ in $\CC$, and under this homeomorphism, $N$ becomes the function $\hat N\colon D(0,1)\to D(0,1)$ given by
	\[\hat N(x)=x^2.\]
\end{theorem}
Notably, $\hat N$ has two fixed points $0$ and $1$, which when pulled back to the map $N\colon\CC\to\CC$ corresponds to the immediate basin containing the fixed point $\alpha$ (as its root) as well as $\infty$. Namely, the immediate basin must contain arbitrary large values! We can say better than this.
\begin{theorem}
	The immediate basin of a given root $\alpha$ has a somewhat large ``wedge'' of $\CC$ as a subset.
\end{theorem}
So here is the idea: fix a polynomial $f$, and set some very large circle $\del B(0,R)$. Then doing a bunch (in fact, $1.11d(\log d)^2$ is enough, where $d\coloneqq\deg f$) of initial guesses evenly distributed around $\del B(0,R)$ will be able to converge to the roots using Newton's method.
\begin{remark}
	This is a miracle! Newton's method was intended to improve approximation of roots locally (close to the roots), but we managed to find all roots globally.
\end{remark}
We close with a couple of further directions.
\begin{itemize}
	\item How does this generalize to systems of polynomial equations?
	\item How does this compare to the secant method to approximate roots?
	\item Is there a story for higher-order approximations (e.g., Halley's method)?
\end{itemize}

\section{Quantum Symmetries, Fusion, Braids, and Categories --- Julia Plavnik}
Professor Plavnik was impressed with the talks yesterday and so made this talk a little harder, just for fun.

\subsection{Symmetries}
Let's start with what a symmetry is.
\begin{itemize}
	\item Colloquially, we think of symmetry as having good proportions, etc.
	\item Art, architecture, and nature are good sources of symmetry.
\end{itemize}
To a mathematician, we encode symmetry inside invariants.
\begin{definition}
	We will say that an object $X$ has a symmetry $f$ if it has a good bijection $f\colon X\to X$.
\end{definition}
\begin{example}
	The triangle has reflections and rotations as its symmetries. Notice that the composition of two symmetries is also a symmetry, and the order of this composition matters.
\end{example}
Observe that these symmetries are ``invertible,'' which verifies automatically (i.e., categorically) that they are bijective.

For some notation, we set $\op{Sym}(X)$ to be the group of symmetries of an object $X$. We are not being terribly formal with this,\footnote{Formally, we need to discuss group actions on a (structured) set $X$.} but we can kind of feel what the definition should mean.
\begin{example}
	The group of symmetries of an equilateral triangle $T$ is $\op{Sym}(T)=S_3$.
\end{example}
\begin{example}
	The group of symmetries of a square $S$ is $\op{Sym}(S)=D_4$, where $D_4$ is the dihedral group with four elements.
\end{example}
We've been talking about groups a lot, so let's give it a definition.
\begin{definition}[Group]
	A \textit{group} is a set $G$ given a binary operation $*\colon G\to G$ satisfying the following axioms.
	\begin{itemize}
		\item Identity: there is $e\in G$ with $e*g=g*e=g$ for all $g\in G$.
		\item Inverse: for all $g\in G$ there exists $g^{-1}$ such that $g*g^{-1}=g^{-1}*g=e$.
	\end{itemize}
\end{definition}
\begin{example}
	The integers $\ZZ$ form a group under addition.
\end{example}
\begin{example}
	We had groups $S_3$ and $D_4$ from before.
\end{example}
\begin{example}
	The group $S_n$ can be generated by transpositions, with certain relations.
\end{example}

\subsection{Braids}
Now let's talk about braids. The braid group $B_n$ is generated by $\sigma_1,\ldots,\sigma_{n-1}$ with the relations
\[\sigma_i\sigma_j=\sigma_j\sigma_i\text{ if }|i-j|<2\]
and
\[\sigma_i\sigma_{i-1}\sigma_i=\sigma_{i-1}\sigma_i\sigma_{i-1}.\]
\begin{remark}
	If we add the relations $\sigma_i^2=\op{id}$ everywhere, then we get $S_n$, so $S_n$ is a natural quotient of $B_n$.
\end{remark}
Formally, we should think about $\sigma_i$ as swapping strands $i$ and $i+1$ in a braid, as follows.
% https://q.uiver.app/?q=WzAsMTQsWzAsMSwiXFxidWxsZXQiXSxbMCwwLCIxIl0sWzIsMCwiaSJdLFszLDAsImkrMSJdLFs1LDAsIm4iXSxbMSwwLCJcXGNkb3RzIl0sWzQsMCwiXFxjZG90cyJdLFswLDMsIlxcYnVsbGV0Il0sWzUsMywiXFxidWxsZXQiXSxbNSwxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzMsMywiXFxidWxsZXQiXSxbMywxLCJcXGJ1bGxldCJdLFsyLDMsIlxcYnVsbGV0Il0sWzAsNywiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbOSw4LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsxMiwxMywiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMTAsMTEsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV1d&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
\[\begin{tikzcd}
	1 & \cdots & i & {i+1} & \cdots & n \\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet
	\arrow[Rightarrow, no head, from=2-1, to=4-1]
	\arrow[Rightarrow, no head, from=2-6, to=4-6]
	\arrow[Rightarrow, no head, from=2-4, to=4-3]
	\arrow[Rightarrow, no head, from=2-3, to=4-4]
\end{tikzcd}\]
Note that there is a ``top'' and ``bottom'' for our strands. In particular, $i$ is going on top of $i+1$ in the above diagram. Multiplication of $\sigma_i$ and $\sigma_j$ consists of putting one braid on top of the other.
% https://q.uiver.app/?q=WzAsMTgsWzAsMSwiXFxidWxsZXQiXSxbMCwwLCIxIl0sWzIsMCwiaSJdLFszLDAsImkrMSJdLFs1LDAsIm4iXSxbMSwwLCJcXGNkb3RzIl0sWzQsMCwiXFxjZG90cyJdLFswLDMsIlxcYnVsbGV0Il0sWzUsMywiXFxidWxsZXQiXSxbNSwxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzMsMywiXFxidWxsZXQiXSxbMywxLCJcXGJ1bGxldCJdLFsyLDMsIlxcYnVsbGV0Il0sWzAsNSwiXFxidWxsZXQiXSxbNSw1LCJcXGJ1bGxldCJdLFsyLDUsIlxcYnVsbGV0Il0sWzMsNSwiXFxidWxsZXQiXSxbMCw3LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs5LDgsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzEyLDEzLCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsxMCwxMSwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbNywxNCwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbOCwxNSwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMTMsMTcsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzExLDE2LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
\[\begin{tikzcd}
	1 & \cdots & i & {i+1} & \cdots & n \\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet
	\arrow[Rightarrow, no head, from=2-1, to=4-1]
	\arrow[Rightarrow, no head, from=2-6, to=4-6]
	\arrow[Rightarrow, no head, from=2-4, to=4-3]
	\arrow[Rightarrow, no head, from=2-3, to=4-4]
	\arrow[Rightarrow, no head, from=4-1, to=6-1]
	\arrow[Rightarrow, no head, from=4-6, to=6-6]
	\arrow[Rightarrow, no head, from=4-3, to=6-4]
	\arrow[Rightarrow, no head, from=4-4, to=6-3]
\end{tikzcd}\]
Here, we have placed $\sigma_i\sigma_i^{-1}$, which we can see we can ``undo'' to get back to the identity. However, $\sigma_i^2$ does a cross-over twice and so cannot go back to the identity.

\subsection{Representation Theory}
We understand linear algebra, so let's try to understand groups by their actions on vector spaces.
\begin{definition}[Representation]
	Given a group $G$, an $n$-dimensional \textit{$k$-representation} is a group homomorphism
	\[\rho\colon G\to\op{GL}_n(k).\]
\end{definition}
More concretely, we are asking for $\rho(gh)=\rho(g)\rho(h)$ for all $g,h\in G$.
\begin{example}
	Take $B_3$, generated by $\{\sigma_1,\sigma_2\}$ with the relation $\sigma_1\sigma_2\sigma_1=\sigma_2\sigma_1\sigma_2$. As such, a representation $\rho\colon B_3\to\op{GL}_n(k)$ amounts to finding matrices $A,B\in\op{GL}_n(k)$ with
	\[ABA=BAB.\]
	We can find a lot of these.
\end{example}
With this in mind, we note the following classification result for representations of $B_3$.
\begin{theorem}
	For $n\le 5$, irreducible $n$-dimensional representations of $B_3$ is given by using an upper triangle matrix $A$ and a lower triangle matrix $B$, where the eigenvalues are in opposite orders: namely, there exists $\lambda_1$ and $\lambda_2$ with
	\[A\sim\begin{bmatrix}
		\lambda_1 & \text{garbage} \\
		0 & \lambda_2
	\end{bmatrix}\qquad\text{and}\qquad B\sim\begin{bmatrix}
		\lambda_2 & 0 \\
		\text{garbage} & \lambda_1
	\end{bmatrix}.\]
\end{theorem}
\begin{remark}
	For $n>5$, things get more complicated.
\end{remark}
There are current research directions in taking $n>5$ or expanding $B_3$. Here are some specific questions.
\begin{itemize}
	\item What happens to representations of $B_k$ for $k>3$?
	\item We can actually describe certain representations of $B_4$ coming from representations of $B_3$. What is their structure?
	\item One can generalize braids beyond three dimensions. What do their representations look like? More specifically, there is a loop braid group whose representations we might want to understand.
\end{itemize}
It turns out that other areas of mathematics and physicists want these representations for reasons of their own.

\subsection{Braided Fusion Categories}
We now turn to studying ``quantum symmetries,'' which means that we want to understand symmetries in quantum mechanics. Ultimately, these describe symmetries of space-time itself.

However, groups are sometimes not the correct definition for our symmetries.
\begin{idea}
	Groups encode symmetries. Tensor categories encode quantum symmetries.
\end{idea}
\begin{remark}
	The field of quantum symmetries, though algebraically motivated, has ties to lots of different mathematics, such as vertex algebras and analysis.
\end{remark}
In particular, we will want quantum groups and Hopf algebras.

For our definition, we will want to define categories. Please, don't be scared---Professor Plavnik really likes them.
\begin{definition}[Category]
	A \textit{category} $\mathcal C$ is a collection of objects $\op{Ob}\mathcal C$ and morphisms $\op{Mor}(-,-)$ between them, satisfying certain coherence laws which we will not state here.
	\begin{itemize}
		\item For each $A\in\op{Ob}\mathcal C$, there is a morphism ${\op{id}_A}\in\op{Mor}(A,A)$.
		\item Given objects $A$, $B$, and $C$, we can compose $f\colon A\to B$ and $g\colon B\to C$ to $(g\circ f)\colon A\to C$.
		\item Given objects $A$ and $B$ with $f\colon A\to B$, we have
		\[f=f\circ{\id_A}={\id_B}\circ f.\]
		\item Lastly, given objects $A$, $B$, $C$, and $D$ with arrows $f\colon A\to B$ and $g\colon B\to C$ and $h\colon D\to A$, we have
		\[(h\circ g)\circ f=h\circ(g\circ f).\]
	\end{itemize}
\end{definition}
\begin{example}
	Familiar categories include $\mathrm{Set}$ (of sets), $\mathrm{Top}$ (of topological spaces), of $\mathrm{Grp}$ (of groups), and $\mathrm{Ab}$ (of abelian groups).
\end{example}
The idea behind category theory is to focus on the arrows between objects in a category instead of the objects themselves. As such, we tend to understand objects up to (perhaps natural) isomorphism instead of equality.

Let's try to build towards ``braided fusion'' categories. Here's the main example.
\begin{example}
	The category $\mathrm{FdVec}_k$ of finite-dimensional $k$-vector spaces have the following nice properties.
	\begin{itemize}
		\item There is a $0$ vector space.
		\item Given ``objects'' $V$ and $W$, we can build $V\oplus W$.
		\item Given ``objects'' $V$ and $W$, the set of morphisms $\op{Mor}(V,W)$ is in fact a finite-dimensional $k$-vector space.
		\item There is a ``smallest'' object $k$, which is the $1$-dimensional vector space. We call it ``simple.''
		\item Namely, any object $V$ is isomorphic to $k^{\oplus n}$ for some nonnegative integer $n$.
		\item There is a product operation $\otimes$ taking $V$ and $W$ to $V\otimes W$.
		\item There is a dual vector space $V^*=\op{Mor}(V,k)$ equipped with maps
		\[\op{ev}_V\colon V^*\otimes V\to k\qquad\text{and}\qquad\op{coev}_V\colon V\otimes V^*\to k.\]
		Evaluation is the canonical map; we won't describe coevaluation in detail.
		\item There is a natural isomorphism $\sigma\colon V\otimes W\to W\otimes V$.
		\item There are some natural associativity laws as well, which satisfy some commuting pentagon which we won't write out here.
	\end{itemize}
\end{example}
And here is our definition.
\begin{definition}[Braided fusion category]
	A \textit{braided fusion category} is essentially a generalization of this example: take all the axioms and write down their natural generalizations, as stated above. We're not going to write these out because it would just be copying and pasting.
\end{definition}
The main point is that we have the following example.
\begin{example}
	The category of finite-dimensional $k$-representations (of a group $G$ such that $(\op{char}k)\nmid\#G$) forms a fusion braided category.
\end{example}
\begin{remark}
	There are some connections between the various axioms of the braided category and some particle interactions in quantum physics. In particular, one can build a fairly concrete dictionary between the two; for example, quantum computing can be seen as applying certain ``braid-like'' operations.
\end{remark}
\begin{remark}
	Given a braided fusion category $\mathcal C$ gives rise to a natural representation of $B_n$: take $X\in\mathcal C$ to $\op{End}_\mathcal C(X^{\otimes n})$ by
	\[\sigma_i\mapsto{\id_X}^{\otimes(i-1)}\otimes\sigma\otimes\id_X^{n-i-3},\]
	where $\sigma\colon A\otimes B\to B\otimes A$ is the swapping isomorphism.
\end{remark}
Here are some the open directions about braided fusion categories.
\begin{itemize}
	\item We might want to classify braided fusion categories, in some suitable sense.
	\item There are some questions about an information-theoretic approach to these categories.
\end{itemize}
Here is some progress.
\begin{definition}[Rank]
	The \textit{rank} of a braided fusion category is the number of simple objects.
\end{definition}
We can classify braided fusion categories up to rank $3$. There is some partial progress up to higher dimensions. Classifying by dimension gives other progress. On the other side of classification, we have been able to build some new braided fusion categories from old; for example, we have had motivations from cohomology, from physics, etc.

\end{document}