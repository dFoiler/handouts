\documentclass{article}
\usepackage[utf8]{inputenc}

\newcommand{\nirpdftitle}{YMC Plenary Talks}
\usepackage{import}
\inputfrom{../notes}{nir}
% \usepackage[backend=biber,
%     style=alphabetic,
%     sorting=ynt
% ]{biblatex}
% \addbibresource{bib.bib}
\setcounter{tocdepth}{2}

\pagestyle{contentpage}

\title{YMC Plenary Talks}
\author{Nir Elber}
\date{August 2022}
\usepackage{graphicx}
\lhead{}
\rhead{\textit{YMC PLENARY TALKS}}

\begin{document}

\maketitle

\tableofcontents

\section{Newton's Method for the Clueless --- Jeff Diller}
Math is more than numbers.
\begin{idea}
	For this talk, math is about solving equations.
\end{idea}
To be specific, we are interested in solving for $x$ solving $f(x)=0$, where $f$ is some specified function (often polynomial).
\begin{example}
	Here are some sets meant to solve equations in $\ZZ[x]$.
	\begin{itemize}
		\item Integers solve equations $x-k\in\ZZ[x]$.
		\item Rational numbers solve equations like $ax+b\in\ZZ[x]$.
		\item Real numbers solve equations like $x^2-2\in\ZZ[x]$.
		\item Complex numbers solve equations like $x^2+1\in\ZZ[x]$.
	\end{itemize}
\end{example}
It will be difficult to ``algebraically'' solving polynomials arbitrarily; this is impossible for degree at least $5$. In this talk, we will be interested in numerical approximations of these solutions. For this, we will use Newton's method.
\begin{remark}
	Newton's method is attributed to Newton, but it's probably due to Simpson. Other mathematicians who have a claim to fame here are Joseph Raphson and Seki Takakazu.
\end{remark}

\subsection{Newton's Method}
We follow the following idea.
\begin{idea}
	If we want to solve $f(x)=0$, guess a solution $x_0$ and pretend $f$ is linear.
\end{idea}
To be a little more technical, we begin with a guess $x_0$, approximate
\[f(x)\approx f(x_0)+f'(x_0)(x-x_0)\]
and then set
\[x_1\coloneqq x_0-\frac{f(x_0)}{f'(x_0)}\]
to be our new guess. Here is the image. 
\begin{center}
	\begin{asy}
		import graph;
		unitsize(3cm);
		real f(real x)
		{
			return x*x*x - x + 0.1;
		}
		draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
		draw(graph(f,-0.6,1.5), blue);

		real a0 = 1.3;
		real a1 = a0 - (a0*a0*a0 - a0) / (3*a0*a0 - 1);

		pair v = (a0, f(a0)) - (a1, 0);
		draw((a1, 0) -- (a1, 0) + 1.8*v, red);

		draw((a0,0) -- (a0,f(a0)), dashed);
		draw((a1,0) -- (a1,f(a1)), dashed);

		dot("$x_0$", (a0,0), S); dot("$x_1$", (a1, 0), S);

		dot("$(x_0,f(x_0))$", (a0, f(a0)), WNW);
		dot("$(x_{1},f(x_{1}))$", (a1, f(a1)), WNW);

		label("\color{red}$y-f(x_0)=f'(x_0)(x-x_0)$", (a1,0) + 1.5*v, W);
	\end{asy}
\end{center}
In practice, we will take our guess $x_1$ and repeat the process, and we will end up well-approximating the root.
\begin{remark}
	This is an example of a ``dynamical system,'' where we have some function $N\colon X\to X$ and ask what happens to the orbits of points $x\in X$. In particular, Newton's method has
	\[N(x)=x-\frac{f(x)}{f'(x)}.\]
\end{remark}
\begin{example}
	With $f(x)=x^2-2$, we have
	\[N(x)\coloneqq\frac12\left(x+\frac2x\right)\]
	as our iteration function. In particular, for any field $K$, we have $N\colon K\to K$.
\end{example}
\begin{example}
	With $f(x)=x^2+1$, we have
	\[N(x)\coloneqq\frac12\left(x+\frac{-1}x\right)\]
	as our iteration function. Because $N\colon\RR\to\RR$, we expect applying Newton's method to guesses starting as real numbers to not be very helpful.
\end{example}
\begin{remark}
	From the perspective of dynamical systems, $f(x)=x^2+1$ is actually very interesting: a random first guess will provide a sequence dense in $\RR$. However, we can find periodic points as well, which is pretty cool.
\end{remark}

\subsection{Efficiency}
To justify Newton's method's efficiency, let's try another method and compare. The idea for our second method is the Intermediate value theorem.
\begin{theorem}[Intermediate value]
	Let $f\colon\RR\to\RR$ be continuous. Then if $f(a)<0$ and $f(b)>0$, there is a root of $f$ between $a$ and $b$.
\end{theorem}
As such, we can imagine taking an interval $[a,b]$, evaluating $f\left(\frac{a+b}2\right)$ and dividing the interval in half depending on its sign. Here is the image.
\begin{center}
	\begin{asy}
		import graph;
		unitsize(3cm);
		real f(real x)
		{
			return x*x*x - x + 0.1;
		}
		draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
		draw(graph(f,-0.6,1.5), blue);

		real a0 = 0.3;
		real b0 = 1.3;
		real a1 = (a0+b0)/2;

		draw((a0,0) -- (a0,f(a0)), dashed);
		draw((a1,0) -- (a1,f(a1)), dashed);
		draw((b0,0) -- (b0,f(b0)), dashed);

		dot("$b_0$", (b0,0), S); dot("$a_0$", (a0, 0), N);
		dot("$a_1$", (a1, 0), N);

		dot("$(a_0,f(a_0))$", (a0, f(a0)), SW);
		dot("$(a_1,f(a_1))$", (a1, f(a1)), SE);
		dot("$(b_0,f(b_0))$", (b0, f(b0)), WNW);
	\end{asy}
\end{center}
This ``Intermediate value theorem'' method uses a single iteration to get a bit of information out of the root. However, Newton's method gets a quadratic improvement on the root!

Let's see this quadratic improvement. Without loss of generality, suppose $f(0)=0$ is our root. Then, using a Taylor series, we have
\[N(x)=x^2\cdot\frac{f''(0)}{f'(0)}+O\left(x^3\right),\]
so with $x$ small we get $N(x)\sim x^2$ to be a quadratic improvement.

\subsection{Random Guess}
One issue with this method is that we need a good first guess. What if we just tried guessing? This idea is due to Cayley.
\begin{example}
	Take $f(x)=x^2-1$. It turns out that, guessing $x_0\in\CC$, we have that $x_n\to1$ if $\op{Re}x_0>0$ and $x_n\to-1$ if $\op{Re}x_0<0$.
\end{example}
\begin{remark}
	In the case of $\op{Re}x_0=0$, this is essentially analogous to trying to solve $f(x)=x^2+1$ over $\RR$.
\end{remark}
The behavior for quadratic polynomials is essentially isomorphic to this behavior for $f(x)=x^2-1$, essentially applying a suitable isometry and dilation.

So what happens with cubic polynomials? Well, the behavior becomes fractal-like and quite complicated, and it stays complicated as one adds more roots. However, we still can gain some control over this. We pick up the following definition.
\begin{definition}
	Let $N\colon\CC\to\CC$ be Newton's method for a given polynomial $f$. The set of all points $z\in\CC$ which approach a given root $\alpha$ is called the \textit{basin} of $\alpha$. The connected component of the basin of $\alpha$ which contains $\alpha$ is called the \textit{immediate basin}.
\end{definition}
\begin{remark}
	All these basins are open: a small perturbed element of the basin will still converge to the same root.
\end{remark}
There is the following result.
\begin{theorem}
	The immediate basin of a given root $\alpha$ is simply connected in $\CC$; i.e., it is homeomorphic to the disk $\overline{B(0,1)}$ of radius $1$ in $\CC$, and under this homeomorphism, $N$ becomes the function $\hat N\colon D(0,1)\to D(0,1)$ given by
	\[\hat N(x)=x^2.\]
\end{theorem}
Notably, $\hat N$ has two fixed points $0$ and $1$, which when pulled back to the map $N\colon\CC\to\CC$ corresponds to the immediate basin containing the fixed point $\alpha$ (as its root) as well as $\infty$. Namely, the immediate basin must contain arbitrary large values! We can say better than this.
\begin{theorem}
	The immediate basin of a given root $\alpha$ has a somewhat large ``wedge'' of $\CC$ as a subset.
\end{theorem}
So here is the idea: fix a polynomial $f$, and set some very large circle $\del B(0,R)$. Then doing a bunch (in fact, $1.11d(\log d)^2$ is enough, where $d\coloneqq\deg f$) of initial guesses evenly distributed around $\del B(0,R)$ will be able to converge to the roots using Newton's method.
\begin{remark}
	This is a miracle! Newton's method was intended to improve approximation of roots locally (close to the roots), but we managed to find all roots globally.
\end{remark}
We close with a couple of further directions.
\begin{itemize}
	\item How does this generalize to systems of polynomial equations?
	\item How does this compare to the secant method to approximate roots?
	\item Is there a story for higher-order approximations (e.g., Halley's method)?
\end{itemize}

\section{Quantum Symmetries, Fusion, Braids, and Categories --- Julia Plavnik}
Professor Plavnik was impressed with the talks yesterday and so made this talk a little harder, just for fun.

\subsection{Symmetries}
Let's start with what a symmetry is.
\begin{itemize}
	\item Colloquially, we think of symmetry as having good proportions, etc.
	\item Art, architecture, and nature are good sources of symmetry.
\end{itemize}
To a mathematician, we encode symmetry inside invariants.
\begin{definition}
	We will say that an object $X$ has a symmetry $f$ if it has a good bijection $f\colon X\to X$.
\end{definition}
\begin{example}
	The triangle has reflections and rotations as its symmetries. Notice that the composition of two symmetries is also a symmetry, and the order of this composition matters.
\end{example}
Observe that these symmetries are ``invertible,'' which verifies automatically (i.e., categorically) that they are bijective.

For some notation, we set $\op{Sym}(X)$ to be the group of symmetries of an object $X$. We are not being terribly formal with this,\footnote{Formally, we need to discuss group actions on a (structured) set $X$.} but we can kind of feel what the definition should mean.
\begin{example}
	The group of symmetries of an equilateral triangle $T$ is $\op{Sym}(T)=S_3$.
\end{example}
\begin{example}
	The group of symmetries of a square $S$ is $\op{Sym}(S)=D_4$, where $D_4$ is the dihedral group with four elements.
\end{example}
We've been talking about groups a lot, so let's give it a definition.
\begin{definition}[Group]
	A \textit{group} is a set $G$ given a binary operation $*\colon G\to G$ satisfying the following axioms.
	\begin{itemize}
		\item Identity: there is $e\in G$ with $e*g=g*e=g$ for all $g\in G$.
		\item Inverse: for all $g\in G$ there exists $g^{-1}$ such that $g*g^{-1}=g^{-1}*g=e$.
	\end{itemize}
\end{definition}
\begin{example}
	The integers $\ZZ$ form a group under addition.
\end{example}
\begin{example}
	We had groups $S_3$ and $D_4$ from before.
\end{example}
\begin{example}
	The group $S_n$ can be generated by transpositions, with certain relations.
\end{example}

\subsection{Braids}
Now let's talk about braids. The braid group $B_n$ is generated by $\sigma_1,\ldots,\sigma_{n-1}$ with the relations
\[\sigma_i\sigma_j=\sigma_j\sigma_i\text{ if }|i-j|<2\]
and
\[\sigma_i\sigma_{i-1}\sigma_i=\sigma_{i-1}\sigma_i\sigma_{i-1}.\]
\begin{remark}
	If we add the relations $\sigma_i^2=\op{id}$ everywhere, then we get $S_n$, so $S_n$ is a natural quotient of $B_n$.
\end{remark}
Formally, we should think about $\sigma_i$ as swapping strands $i$ and $i+1$ in a braid, as follows.
% https://q.uiver.app/?q=WzAsMTQsWzAsMSwiXFxidWxsZXQiXSxbMCwwLCIxIl0sWzIsMCwiaSJdLFszLDAsImkrMSJdLFs1LDAsIm4iXSxbMSwwLCJcXGNkb3RzIl0sWzQsMCwiXFxjZG90cyJdLFswLDMsIlxcYnVsbGV0Il0sWzUsMywiXFxidWxsZXQiXSxbNSwxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzMsMywiXFxidWxsZXQiXSxbMywxLCJcXGJ1bGxldCJdLFsyLDMsIlxcYnVsbGV0Il0sWzAsNywiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbOSw4LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsxMiwxMywiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMTAsMTEsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV1d&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
\[\begin{tikzcd}
	1 & \cdots & i & {i+1} & \cdots & n \\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet
	\arrow[Rightarrow, no head, from=2-1, to=4-1]
	\arrow[Rightarrow, no head, from=2-6, to=4-6]
	\arrow[Rightarrow, no head, from=2-4, to=4-3]
	\arrow[Rightarrow, no head, from=2-3, to=4-4]
\end{tikzcd}\]
Note that there is a ``top'' and ``bottom'' for our strands. In particular, $i$ is going on top of $i+1$ in the above diagram. Multiplication of $\sigma_i$ and $\sigma_j$ consists of putting one braid on top of the other.
% https://q.uiver.app/?q=WzAsMTgsWzAsMSwiXFxidWxsZXQiXSxbMCwwLCIxIl0sWzIsMCwiaSJdLFszLDAsImkrMSJdLFs1LDAsIm4iXSxbMSwwLCJcXGNkb3RzIl0sWzQsMCwiXFxjZG90cyJdLFswLDMsIlxcYnVsbGV0Il0sWzUsMywiXFxidWxsZXQiXSxbNSwxLCJcXGJ1bGxldCJdLFsyLDEsIlxcYnVsbGV0Il0sWzMsMywiXFxidWxsZXQiXSxbMywxLCJcXGJ1bGxldCJdLFsyLDMsIlxcYnVsbGV0Il0sWzAsNSwiXFxidWxsZXQiXSxbNSw1LCJcXGJ1bGxldCJdLFsyLDUsIlxcYnVsbGV0Il0sWzMsNSwiXFxidWxsZXQiXSxbMCw3LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFs5LDgsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzEyLDEzLCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dLFsxMCwxMSwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbNywxNCwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbOCwxNSwiIiwwLHsibGV2ZWwiOjIsInN0eWxlIjp7ImhlYWQiOnsibmFtZSI6Im5vbmUifX19XSxbMTMsMTcsIiIsMCx7ImxldmVsIjoyLCJzdHlsZSI6eyJoZWFkIjp7Im5hbWUiOiJub25lIn19fV0sWzExLDE2LCIiLDAseyJsZXZlbCI6Miwic3R5bGUiOnsiaGVhZCI6eyJuYW1lIjoibm9uZSJ9fX1dXQ==&macro_url=https%3A%2F%2Fraw.githubusercontent.com%2FdFoiler%2Fnotes%2Fmaster%2Fnir.tex
\[\begin{tikzcd}
	1 & \cdots & i & {i+1} & \cdots & n \\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet \\
	\\
	\bullet && \bullet & \bullet && \bullet
	\arrow[Rightarrow, no head, from=2-1, to=4-1]
	\arrow[Rightarrow, no head, from=2-6, to=4-6]
	\arrow[Rightarrow, no head, from=2-4, to=4-3]
	\arrow[Rightarrow, no head, from=2-3, to=4-4]
	\arrow[Rightarrow, no head, from=4-1, to=6-1]
	\arrow[Rightarrow, no head, from=4-6, to=6-6]
	\arrow[Rightarrow, no head, from=4-3, to=6-4]
	\arrow[Rightarrow, no head, from=4-4, to=6-3]
\end{tikzcd}\]
Here, we have placed $\sigma_i\sigma_i^{-1}$, which we can see we can ``undo'' to get back to the identity. However, $\sigma_i^2$ does a cross-over twice and so cannot go back to the identity.

\subsection{Representation Theory}
We understand linear algebra, so let's try to understand groups by their actions on vector spaces.
\begin{definition}[Representation]
	Given a group $G$, an $n$-dimensional \textit{$k$-representation} is a group homomorphism
	\[\rho\colon G\to\op{GL}_n(k).\]
\end{definition}
More concretely, we are asking for $\rho(gh)=\rho(g)\rho(h)$ for all $g,h\in G$.
\begin{example}
	Take $B_3$, generated by $\{\sigma_1,\sigma_2\}$ with the relation $\sigma_1\sigma_2\sigma_1=\sigma_2\sigma_1\sigma_2$. As such, a representation $\rho\colon B_3\to\op{GL}_n(k)$ amounts to finding matrices $A,B\in\op{GL}_n(k)$ with
	\[ABA=BAB.\]
	We can find a lot of these.
\end{example}
With this in mind, we note the following classification result for representations of $B_3$.
\begin{theorem}
	For $n\le 5$, irreducible $n$-dimensional representations of $B_3$ is given by using an upper triangle matrix $A$ and a lower triangle matrix $B$, where the eigenvalues are in opposite orders: namely, there exists $\lambda_1$ and $\lambda_2$ with
	\[A\sim\begin{bmatrix}
		\lambda_1 & \text{garbage} \\
		0 & \lambda_2
	\end{bmatrix}\qquad\text{and}\qquad B\sim\begin{bmatrix}
		\lambda_2 & 0 \\
		\text{garbage} & \lambda_1
	\end{bmatrix}.\]
\end{theorem}
\begin{remark}
	For $n>5$, things get more complicated.
\end{remark}
There are current research directions in taking $n>5$ or expanding $B_3$. Here are some specific questions.
\begin{itemize}
	\item What happens to representations of $B_k$ for $k>3$?
	\item We can actually describe certain representations of $B_4$ coming from representations of $B_3$. What is their structure?
	\item One can generalize braids beyond three dimensions. What do their representations look like? More specifically, there is a loop braid group whose representations we might want to understand.
\end{itemize}
It turns out that other areas of mathematics and physicists want these representations for reasons of their own.

\subsection{Braided Fusion Categories}
We now turn to studying ``quantum symmetries,'' which means that we want to understand symmetries in quantum mechanics. Ultimately, these describe symmetries of space-time itself.

However, groups are sometimes not the correct definition for our symmetries.
\begin{idea}
	Groups encode symmetries. Tensor categories encode quantum symmetries.
\end{idea}
\begin{remark}
	The field of quantum symmetries, though algebraically motivated, has ties to lots of different mathematics, such as vertex algebras and analysis.
\end{remark}
In particular, we will want quantum groups and Hopf algebras.

For our definition, we will want to define categories. Please, don't be scared---Professor Plavnik really likes them.
\begin{definition}[Category]
	A \textit{category} $\mathcal C$ is a collection of objects $\op{Ob}\mathcal C$ and morphisms $\op{Mor}(-,-)$ between them, satisfying certain coherence laws which we will not state here.
	\begin{itemize}
		\item For each $A\in\op{Ob}\mathcal C$, there is a morphism ${\op{id}_A}\in\op{Mor}(A,A)$.
		\item Given objects $A$, $B$, and $C$, we can compose $f\colon A\to B$ and $g\colon B\to C$ to $(g\circ f)\colon A\to C$.
		\item Given objects $A$ and $B$ with $f\colon A\to B$, we have
		\[f=f\circ{\id_A}={\id_B}\circ f.\]
		\item Lastly, given objects $A$, $B$, $C$, and $D$ with arrows $f\colon A\to B$ and $g\colon B\to C$ and $h\colon D\to A$, we have
		\[(h\circ g)\circ f=h\circ(g\circ f).\]
	\end{itemize}
\end{definition}
\begin{example}
	Familiar categories include $\mathrm{Set}$ (of sets), $\mathrm{Top}$ (of topological spaces), of $\mathrm{Grp}$ (of groups), and $\mathrm{Ab}$ (of abelian groups).
\end{example}
The idea behind category theory is to focus on the arrows between objects in a category instead of the objects themselves. As such, we tend to understand objects up to (perhaps natural) isomorphism instead of equality.

Let's try to build towards ``braided fusion'' categories. Here's the main example.
\begin{example}
	The category $\mathrm{FdVec}_k$ of finite-dimensional $k$-vector spaces have the following nice properties.
	\begin{itemize}
		\item There is a $0$ vector space.
		\item Given ``objects'' $V$ and $W$, we can build $V\oplus W$.
		\item Given ``objects'' $V$ and $W$, the set of morphisms $\op{Mor}(V,W)$ is in fact a finite-dimensional $k$-vector space.
		\item There is a ``smallest'' object $k$, which is the $1$-dimensional vector space. We call it ``simple.''
		\item Namely, any object $V$ is isomorphic to $k^{\oplus n}$ for some nonnegative integer $n$.
		\item There is a product operation $\otimes$ taking $V$ and $W$ to $V\otimes W$.
		\item There is a dual vector space $V^*=\op{Mor}(V,k)$ equipped with maps
		\[\op{ev}_V\colon V^*\otimes V\to k\qquad\text{and}\qquad\op{coev}_V\colon V\otimes V^*\to k.\]
		Evaluation is the canonical map; we won't describe coevaluation in detail.
		\item There is a natural isomorphism $\sigma\colon V\otimes W\to W\otimes V$.
		\item There are some natural associativity laws as well, which satisfy some commuting pentagon which we won't write out here.
	\end{itemize}
\end{example}
And here is our definition.
\begin{definition}[Braided fusion category]
	A \textit{braided fusion category} is essentially a generalization of this example: take all the axioms and write down their natural generalizations, as stated above. We're not going to write these out because it would just be copying and pasting.
\end{definition}
The main point is that we have the following example.
\begin{example}
	The category of finite-dimensional $k$-representations (of a group $G$ such that $(\op{char}k)\nmid\#G$) forms a fusion braided category.
\end{example}
\begin{remark}
	There are some connections between the various axioms of the braided category and some particle interactions in quantum physics. In particular, one can build a fairly concrete dictionary between the two; for example, quantum computing can be seen as applying certain ``braid-like'' operations.
\end{remark}
\begin{remark}
	Given a braided fusion category $\mathcal C$ gives rise to a natural representation of $B_n$: take $X\in\mathcal C$ to $\op{End}_\mathcal C(X^{\otimes n})$ by
	\[\sigma_i\mapsto{\id_X}^{\otimes(i-1)}\otimes\sigma\otimes\id_X^{n-i-3},\]
	where $\sigma\colon A\otimes B\to B\otimes A$ is the swapping isomorphism.
\end{remark}
Here are some the open directions about braided fusion categories.
\begin{itemize}
	\item We might want to classify braided fusion categories, in some suitable sense.
	\item There are some questions about an information-theoretic approach to these categories.
\end{itemize}
Here is some progress.
\begin{definition}[Rank]
	The \textit{rank} of a braided fusion category is the number of simple objects.
\end{definition}
We can classify braided fusion categories up to rank $3$. There is some partial progress up to higher dimensions. Classifying by dimension gives other progress. On the other side of classification, we have been able to build some new braided fusion categories from old; for example, we have had motivations from cohomology, from physics, etc.

\section{Interpolation Problems for Curves --- Isabel Vogt}
Let's start at the (very) beginning. Here is our question.
\begin{ques}
	When is there a curve (with some specified properties) which goes through some given points.
\end{ques}
\begin{example}
	We can choose a curve
	\begin{center}
		\begin{asy}
			unitsize(1cm);
			dot((1,0) ^^ (0,1) ^^ (-2,0) ^^ (-2,-0.5));
		\end{asy}
	\end{center}
	as follows.
	\begin{center}
		\begin{asy}
			unitsize(1cm);
			dot((1,0) ^^ (0,1) ^^ (-2,0) ^^ (-2,-0.5));
			draw((1,0) .. (0,1) .. (-2,0) .. (-2,-0.5));
		\end{asy}
	\end{center}
\end{example}

\subsection{Classical Results}
Here are some basic examples.
\begin{example}
	There is a line between two points.
\end{example}
\begin{theorem}[Euclid]
	There is a circle between three noncollinear points.
\end{theorem}
Note that the circle is requiring us to work with points in ``general position.'' With this in mind, we have the following more general result.
\begin{theorem}[Pappus] \label{thm:pappus}
	Any five points in general position has a conic section.
\end{theorem}
Providing an actual synthetic construction of this conic is quite difficult. However, it is still possible.

\subsection{Classical Algebraic Geometry}
Instead of using compasses and straightedges, we began using equations. This allows us to prove \autoref{thm:pappus} fairly easily. To begin, we define a conic.
\begin{definition}
	A \textit{conic} $C$ is a curve specified by an equation
	\[C\colon ax^2+bxy+cy^2+dx+ey+f=0.\]
\end{definition}
Namely, $C(k)$ consists of the set of pairs $(x,y)\in k^2$ which satisfy the equation given by $C$.

So here is our theorem.
\begin{proof}[Proof of \autoref{thm:pappus}]
	Fix our conic as
	\[C\colon ax^2+bxy+cy^2+dx+ey+f=0.\]
	Now, given our five points $(p_i,q_i)$ for $1\le i\le 5$, we are going to get a system of equations
	\[ap_i^2+bp_iq_i+cq_i^2+dp_i+eq_i+f=0\qquad\text{for}\qquad1\le i\le 5.\]
	However, these equations are linear in $(a,b,c,d,e,f)$, so we can simply solve for our conic directly! Namely, we want
	\[\begin{bmatrix}
		a \\ b \\ c \\ d \\ e \\ f
	\end{bmatrix}\in\ker\begin{bmatrix}
		p_i^2 & p_iq_i & q_i^2 & p_i & q_i & 1
	\end{bmatrix},\]
	where the matrix at the right has five rows. However, we have five rows and six columns, so it has nonzero kernel, allowing us to find our nonzero conic given by $(a,b,c,d,e,f)$. This finishes the proof.
\end{proof}
\begin{remark}
	In fact, if the points are in general position we expect the matrix to have kernel with dimension exactly $1$, so the $(a,b,c,d,e,f)$ defines a unique conic.
\end{remark}
\begin{remark}
	Realizing or space of conics as a kernel allows us to give it a vector space structure.
\end{remark}
With this approach, we actually have the following guiding philosophy.
\begin{idea}
	A general curve of degree $d$ can interpolate $n-1$ points, where $n$ is the number of coefficients in a general degree-$d$ curve.
\end{idea}
To count this number of coefficients, we note that there are $k+1$ different monomials $x^ay^b$ with $a+b=k$, so the total number of monomials with degree at most $d$ is
\[\sum_{i=0}^{d}(i+1)=\frac{(d+1)(d+2)}2=\binom{d+2}2,\]
so we need
\[\binom{d+2}2-1=\frac{d(d+3)}2\]
total points.
\begin{remark}
	In fact, we have not used the fact that our points need to be in general position. This in particular comes in to make sure that our plane curve has degree actually $d$ and not less.
\end{remark}
Anyway, here is the result we proved.
\begin{theorem}[Cramer] \label{thm:cramer}
	A plane curve of degree $d$ can interpolate $\frac{d(d+3)}2$ points, and for points of general position then this curve is unique.
\end{theorem}
\begin{remark}
	We have been a little fast and loose about what ``general position'' means; to be technical, we simply ask for the result in an open dense subset.
\end{remark}

\subsection{Waring/Lagrange Interpolation}
Here is a different source of interpolation problem: we want a polynomial $f$ of degree (at most) $d$ with $f(p_i)=q_i$ for $n$ given points $(p_i,q_i)$ where $1\le i\le n$.
\begin{example}
	We can fit the points
	\begin{center}
		\begin{asy}
			import graph;
			unitsize(1.5cm);
			real f(real x)
			{
				return x*x*x - x + 0.1 + 0;
			}
			draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
			//draw(graph(f,-0.6,1.5), blue);
			dot((0,f(0)) ^^ (1,f(1)) ^^ (0.7,f(0.7)) ^^ (1.3,f(1.3)));
		\end{asy}
	\end{center}
	as follows.
	\begin{center}
		\begin{asy}
			import graph;
			unitsize(1.5cm);
			real f(real x)
			{
				return x*x*x - x + 0.1 + 0;
			}
			draw((-0.6,0)--(2,0)); draw((0,-0.6)--(0,2));
			draw(graph(f,-0.6,1.5), blue);
			dot((0,f(0)) ^^ (1,f(1)) ^^ (0.7,f(0.7)) ^^ (1.3,f(1.3)));
		\end{asy}
	\end{center}
\end{example}
Essentially the same approach that we had before will work: namely we set
\[f(x)=\sum_{i=0}^{d}a_ix^i\]
and want to solve
\[\begin{bmatrix}
	p_1^d & p_1^{d-1} & \cdots & p_1 & 1 \\
	\vdots & \vdots & \ddots & \vdots & \vdots \\
	p_n^d & p_n^{d-1} & \cdots & p_n & 1
\end{bmatrix}\begin{bmatrix}
	a_d \\
	\vdots \\
	a_0
\end{bmatrix}=\begin{bmatrix}
	q_1 \\
	\vdots \\
	p_n
\end{bmatrix}.\]
Then this can be solved provided that $n\le d+1$. So we have the following result.
\begin{theorem}[Waring--Lagrange]
	There exists a unique polynomial $f$ of degree at most $d$ interpolating $d+1$ points.
\end{theorem}
\begin{remark}
	The above approach should be attributed to Waring, but we gave it to Lagrange.
\end{remark}

\subsection{Reed--Solomon Error-Correcting Codes}
Here is a nice application problem: suppose we want to send a message consisting of the possibly large number of characters $p_1,\ldots,p_n$, but we are going through a lossy channel. One naive way to detect if an error has occurred is to send
\[p_1,\ldots,p_n,p_1,\ldots,p_n.\]
Namely, in our two copies we can detect the error if there is a difference between the copies. If we want to correct the error, we should send three copies, using the third copy to break ties about what the error is.

The issue is that we are sending $3n$ terms to correct a single error in a message of length $n$. However, we claim that we can do this same task in $n+2$ terms instead of $3n$. Here is the magic. Construct the polynomial
\[f(x)=p_1x^{n-1}+p_2x^{n-2}+\cdots+p_{n-1}x+p_n.\]
Then we can send $f(1),\ldots,f(n)$ to determine $f$ uniquely, and they allow us to send the message.

However, if we want to detect an error, we can send $f(n+1)$, and if we cannot make a single polynomial $f$ from our data, then we have an error. Namely, we have over-determined our polynomial, so an error is not expected to exist! A single error is correcting this way.
\begin{remark}
	Of course, there might be lots of errors which push us back to being a polynomial again. If we want to detect $k$ errors, then we should send
	\[f(n+1),\ldots,f(n+k)\]
	as well.
\end{remark}
If we want to do error correction, then we can correct for a single error by sending $f(n+1)$ and $f(n+2)$: if we cannot make a polynomial, then we simply choose the largest subset of $n$ inputs which do make a polynomial, and the point we have left over is the one with the error, and we can correct it according to our solved polynomial.
\begin{remark}
	In practice, we use finite fields for our outputs of $f$. This ensures that we are not transmitting numbers which are huge.
\end{remark}

\subsection{Further Directions}
Here are some further directions for interpolations.
\begin{itemize}
	\item We can try to interpolate higher-dimensional varieties with points again. For example, with a quadric surface given by equation
	\[Q\colon ax^2+bxy+cy^2+dyz+ez^2+fzx+gx+hy+iz+j\]
	requires nine points to specify.
	\item Here is a different sort of problem: working in $r$-dimensional space, we can specify curves of degree $d$ (meaning that any cross-section with a $1$-dimensional hyperplane should intersect at $d$ points) as well as the genus $g$.
\end{itemize}
Going in the second direction, which is a little more interesting, we can ask the following question.
\begin{ques} \label{ques:genus}
	Having specified the genus $g$ and dimension $d$ of our variety living in $r$-space, how many points in general position can we interpolate?
\end{ques}
\begin{example}
	A degree-$d$ plane curve can interpolate $\frac{d(d+3)}2$ points. This was \autoref{thm:cramer}.
\end{example}
However, the approach we used for \autoref{thm:cramer} will not be so easy anymore because it is difficult to specify these varieties as equations like we did there.

Nonetheless, let's try to reframe the proof of \autoref{thm:cramer}: the dimension of the space $X$ of degree-$d$ plane curves is $\frac{d(d+3)}2$ from our monomial computations. To specify our interpolation, we make a space $X_n$ of dimension
\[\frac{d(d+3)}2+n\]
as the curves themselves with $n$ points marked on the curve. There is a natural projection $X_n\onto X$ by going down to the curve, and our fibers have dimension $n$ given by the points themselves.

Alternatively, there is a natural map $X\onto k^{2n}$ down to the points themselves. So we are asking for this map
\[X\onto k^{2n}\]
to be ``dominant'' in the sense that we should hit all points in general position. Well, one bound we can use for this question is to ask for the dimension of the source space $\frac{d(d+3)}2+n$ to be at least $2n$, which shows that we are requiring
\[\frac{d(d+3)}2+n\ge 2n,\]
forcing $\frac{d(d+3)}2\ge n$. This is one side of \autoref{thm:cramer}.

Let's now generalize this. For \autoref{ques:genus}, we will want to know the dimension
\[\dim\mathcal M(g,d,r)\]
of varieties with genus $g$, of dimension-$d$, living in $r$-space. This turns out to be incredibly difficult, but here is something partial we can say.
\begin{theorem}[Brill--Noether]
	If
	\[\rho(g,r,d)=g-(r+1)(g-d+r)\ge0,\]
	then there is a unique component $\mathcal M(g,r,d)^\circ\subseteq\mathcal M(g,r,d)$ containing ``general'' genus-$g$ curves; we call these \textit{Brill--Noether curves}. In fact,
	\[\dim\mathcal M(g,r,d)^\circ=(r+1)d-(r-3)(g-1).\]
\end{theorem}
So let's state our conjecture, which is that the lower bound given by the above approach is sharp.
\begin{conj}
	A Brill--Noether curve of degree $d$ and genus $g$ in $r$-space interpolate $n$ points in general position if and only if
	\[m\le\dim\mathcal M(g,r,d,n)^\circ=(r+1)d-(r-3)(g-1)+n.\]
\end{conj}
\begin{example}
	With $d=6$ and $g=4$ and $r=3$, we predict that we can interpolate $12$ general points. However, it turns out that all these varieties lie in a quadric surface, so actually we could only use $9$ points.
\end{example}
However, it turns out that this is one of only two counterexamples in $r=3$.
\begin{theorem}[Vogt, 2018]
	A Brill--Noether curve of degree $d$ and genus $g$ in $3$-space interpolate $n$ points in general position if and only if
	\[m\le\dim\mathcal M(g,r,d,n)^\circ=(r+1)d-(r-3)(g-1)+n,\]
	provided that $(d,g)\notin\{(5,2),(6,4)\}$.
\end{theorem}
It turns out that there are no counterexamples at dimension $r=4$. Here is the full result.
\begin{theorem}[Vogt--Larson, 2022]
	A Brill--Noether curve of degree $d$ and genus $g$ in $r$-space interpolate $n$ points in general position if and only if
	\[m\le\dim\mathcal M(g,r,d,n)^\circ=(r+1)d-(r-3)(g-1)+n,\]
	provided that
	\[(d,g,r)\notin\{(5,2,3),(6,4,3),(7,2,5),(10,6,5)\}.\]
\end{theorem}
\begin{remark}
	This result is very hard to prove! For a reasoning why, note that whatever proof you write down must account for the counterexamples.
\end{remark}
The moral of the story is that sometimes you start a project in 2016 and only finish it in 2022.

\end{document}